{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AYuX015818qd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.0002\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "r4mMNBww2FVI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
        "])\n",
        "mnist_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "data_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q-EhZGp2NzS",
        "outputId": "53a8dce4-be1d-4a76-8500-9e7a19394576"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.91MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 15.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.44MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z).view(-1, 1, 28, 28)\n"
      ],
      "metadata": {
        "id": "Cx0CDVMq2PjY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the 28x28 image to 784\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "c1M0gxSY2VBI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "generator = Generator(latent_dim)\n",
        "discriminator = Discriminator()\n"
      ],
      "metadata": {
        "id": "6n1Ue_oG2XeY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizers\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "YIT-u8vD2ZlY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real_images, _ in data_loader:\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        real_images = real_images.to(torch.float32)\n",
        "        real_loss = loss_fn(discriminator(real_images), real_labels)\n",
        "\n",
        "        z = torch.randn(batch_size, latent_dim)\n",
        "        fake_images = generator(z)\n",
        "        fake_loss = loss_fn(discriminator(fake_images.detach()), fake_labels)\n",
        "\n",
        "        d_loss = real_loss + fake_loss\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, latent_dim)\n",
        "        fake_images = generator(z)\n",
        "        g_loss = loss_fn(discriminator(fake_images), real_labels)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print losses every epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Generate and save images every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        z = torch.randn(16, latent_dim)\n",
        "        samples = generator(z).detach()\n",
        "        samples = (samples + 1) / 2  # Rescale to [0, 1]\n",
        "        grid = torch.cat([torch.cat([samples[i * 4 + j] for j in range(4)], dim=2) for i in range(4)], dim=1)\n",
        "        plt.imshow(grid.numpy().squeeze(), cmap=\"gray\")\n",
        "        plt.title(f\"Generated Images at Epoch {epoch+1}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW31q0Nn2by5",
        "outputId": "86bc6e29-aab0-4fd4-c2a0-242d39bbbd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, D_loss: 0.1919, G_loss: 3.7536\n",
            "Epoch 2/50, D_loss: 0.7403, G_loss: 2.2801\n",
            "Epoch 3/50, D_loss: 0.1292, G_loss: 3.4361\n",
            "Epoch 4/50, D_loss: 0.5410, G_loss: 5.9840\n",
            "Epoch 5/50, D_loss: 0.0934, G_loss: 5.5936\n",
            "Epoch 6/50, D_loss: 0.1315, G_loss: 6.4135\n",
            "Epoch 7/50, D_loss: 0.0723, G_loss: 6.3991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECP8-ZAV2gGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}